{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed59f13d",
   "metadata": {},
   "source": [
    "## High Resolution Image generation Code for Dr.Sabato's Honeywell Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba24874e",
   "metadata": {},
   "source": [
    "### For FFN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702dca17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import layers\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create output directory for high-resolution images\n",
    "output_folder = \"high_res_ffn_images\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Part 1: Train and save the FFN model\n",
    "def train_and_save_model():\n",
    "    print(\"Loading training and test data...\")\n",
    "    Train_df = pd.read_csv(r\"C:\\Users\\G_Modak\\Desktop\\Honeywell Project\\FEMap Models\\FAZ_FEModel Files\\Thermal analysis_temp on each nodes_different thickness inserts_cropped_100x100_please1.csv\")\n",
    "    Test_df = pd.read_csv(r\"C:\\Users\\G_Modak\\Desktop\\Honeywell Project\\FEMap Models\\FAZ_FEModel Files\\Thermal analysis_temp on each nodes_different thickness inserts_cropped_100x100_please1.csv\")\n",
    "\n",
    "    # Normalize the data\n",
    "    max_temp = 333.0\n",
    "    Train = pd.DataFrame(Train_df).to_numpy() / max_temp\n",
    "    Test = pd.DataFrame(Test_df).to_numpy() / max_temp\n",
    "\n",
    "    # Create sparse mask with 5% of the data\n",
    "    mask_ratio = 0.05\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    mask = np.random.binomial(1, mask_ratio, Train[0,:].shape)\n",
    "\n",
    "    # Create sparse input data\n",
    "    Train_masked = Train * mask\n",
    "    Test_masked = Test * mask\n",
    "\n",
    "    # Build the autoencoder model\n",
    "    input_data = keras.Input(shape=(10000,))\n",
    "    encoded = layers.Dense(128, activation='relu')(input_data)\n",
    "    encoded = layers.Dense(64, activation='relu')(encoded)\n",
    "    encoded = layers.Dense(32, activation='relu')(encoded)\n",
    "    decoded = layers.Dense(64, activation='relu')(encoded)\n",
    "    decoded = layers.Dense(128, activation='relu')(decoded)\n",
    "    decoded = layers.Dense(10000, activation='sigmoid')(decoded)\n",
    "\n",
    "    # Create and compile the autoencoder\n",
    "    autoencoder = keras.Model(input_data, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "    # Train the autoencoder\n",
    "    print(\"Training the FFN autoencoder model...\")\n",
    "    history = autoencoder.fit(\n",
    "        Train_masked, Train,\n",
    "        epochs=100,\n",
    "        batch_size=4,\n",
    "        shuffle=True,\n",
    "        validation_data=(Test_masked, Test),\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Save the model\n",
    "    model_filename = f'ffn_autoencoder_{mask_ratio*100}percent.keras'\n",
    "    autoencoder.save(model_filename)\n",
    "    print(f\"Model saved as {model_filename}\")\n",
    "\n",
    "    # Plot and save loss history\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('FFN Autoencoder Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    loss_plot_filename = f'autoencoder_loss_FFN_{mask_ratio*100}%.png'\n",
    "    plt.savefig(loss_plot_filename)\n",
    "    plt.close()\n",
    "    print(f\"Loss plot saved as {loss_plot_filename}\")\n",
    "\n",
    "    return model_filename, mask_ratio, max_temp\n",
    "\n",
    "# Part 2: Generate high-resolution images at every 100th timestep\n",
    "def generate_high_resolution_images(model_filename, mask_ratio, max_temp, vmin=295.0, vmax=335.0, dpi=600):\n",
    "    print(\"Loading data for image generation...\")\n",
    "    # Load the data\n",
    "    df = pd.read_csv(r\"C:\\Users\\G_Modak\\Desktop\\Honeywell Project\\FEMap Models\\FAZ_FEModel Files\\Thermal analysis_temp on each nodes_different thickness inserts_cropped_100x100_please1.csv\")\n",
    "    \n",
    "    # Load the model\n",
    "    print(f\"Loading trained model from {model_filename}...\")\n",
    "    autoencoder = load_model(model_filename)\n",
    "    \n",
    "    # Normalize the data\n",
    "    data = pd.DataFrame(df).to_numpy() / max_temp\n",
    "    \n",
    "    # Create sparse mask with same ratio as training\n",
    "    np.random.seed(42)  # For reproducibility - use same seed as training\n",
    "    mask = np.random.binomial(1, mask_ratio, data[0,:].shape)\n",
    "    \n",
    "    # Create sparse input data\n",
    "    data_masked = data * mask\n",
    "    \n",
    "    # Generate predictions\n",
    "    print(\"Generating predictions with the model...\")\n",
    "    predicted_data = autoencoder.predict(data_masked)\n",
    "    \n",
    "    # Define timesteps to use\n",
    "    # timesteps = range(900, 1000, 100)  # Every 100th timestep from 0 to 900\n",
    "    timesteps = [0,100,200,300,400,500,600,700,800,900,999]\n",
    "    \n",
    "    # Set grid dimensions\n",
    "    grid_size = 100  # 100x100 grid\n",
    "    \n",
    "    # Process each selected timestep\n",
    "    for timestep in timesteps:\n",
    "        print(f\"Generating high-resolution images for timestep {timestep}...\")\n",
    "        \n",
    "        # Get original, masked, and reconstructed data for this timestep\n",
    "        original = data[timestep].reshape(grid_size, grid_size) * max_temp\n",
    "        masked = data_masked[timestep].reshape(grid_size, grid_size) * max_temp\n",
    "        reconstructed = predicted_data[timestep].reshape(grid_size, grid_size) * max_temp\n",
    "        \n",
    "        # Calculate relative error\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            relative_error = (original - reconstructed) / original\n",
    "            # Handle division by zero or very small values\n",
    "            relative_error = np.where(np.isfinite(relative_error), relative_error, 0)\n",
    "        \n",
    "        # Calculate error bounds for better visualization\n",
    "        p05 = np.percentile(relative_error.flatten(), 5)\n",
    "        p95 = np.percentile(relative_error.flatten(), 95)\n",
    "        error_max = max(abs(p05), abs(p95))\n",
    "        \n",
    "        # Create a 2x2 figure for all four images\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(20, 16), dpi=dpi)\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        # Set titles for each subplot\n",
    "        titles = [\"Original Temperature Field\", \n",
    "                f\"Sparse Data ({mask_ratio*100}%)\",\n",
    "                \"FFN Reconstructed Field\", \n",
    "                \"Relative Error ((Original-Reconstructed)/Original)\"]\n",
    "        \n",
    "        # Plot each image\n",
    "        im0 = axes[0].imshow(original, cmap='viridis', vmin=vmin, vmax=vmax)\n",
    "        im1 = axes[1].imshow(masked, cmap='viridis', vmin=vmin, vmax=vmax)\n",
    "        im2 = axes[2].imshow(reconstructed, cmap='viridis', vmin=vmin, vmax=vmax)\n",
    "        im3 = axes[3].imshow(relative_error, cmap='viridis', vmin=-error_max, vmax=error_max)\n",
    "        \n",
    "        # Add colorbars\n",
    "        cbar0 = fig.colorbar(im0, ax=axes[0])\n",
    "        cbar0.set_label('Temperature (K)', fontsize=12)\n",
    "        cbar1 = fig.colorbar(im1, ax=axes[1])\n",
    "        cbar1.set_label('Temperature (K)', fontsize=12)\n",
    "        cbar2 = fig.colorbar(im2, ax=axes[2])\n",
    "        cbar2.set_label('Temperature (K)', fontsize=12)\n",
    "        cbar3 = fig.colorbar(im3, ax=axes[3])\n",
    "        cbar3.set_label('Relative Temperature Difference', fontsize=12)\n",
    "        \n",
    "        # Set titles and labels\n",
    "        for i, title in enumerate(titles):\n",
    "            axes[i].set_title(title, fontsize=14)\n",
    "            axes[i].set_xlabel('Space (x)', fontsize=12)\n",
    "            axes[i].set_ylabel('Space (y)', fontsize=12)\n",
    "        \n",
    "        # Add main title\n",
    "        plt.suptitle(f\"FFN Thermal Field Reconstruction (Timestep: {timestep})\", fontsize=16)\n",
    "        \n",
    "        # Adjust spacing\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        \n",
    "        # Save combined image\n",
    "        combined_file_path = os.path.join(output_folder, f\"ffn_combined_view_timestep_{timestep}.png\")\n",
    "        plt.savefig(combined_file_path, dpi=dpi, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Saved combined image to {combined_file_path}\")\n",
    "        \n",
    "        # Now generate individual high-resolution images for each component\n",
    "        components = {\n",
    "            'original': (original, 'Original Temperature Field', vmin, vmax, 'Temperature (K)'),\n",
    "            'sparse': (masked, f'Sparse Data ({mask_ratio*100}%)', vmin, vmax, 'Temperature (K)'),\n",
    "            'reconstructed': (reconstructed, 'FFN Reconstructed Field', vmin, vmax, 'Temperature (K)'),\n",
    "            'relative_error': (relative_error, 'Relative Error ((Original-Reconstructed)/Original)', \n",
    "                            -error_max, error_max, 'Relative Temperature Difference')\n",
    "        }\n",
    "        \n",
    "        for comp_name, (data_array, title, v_min, v_max, cbar_label) in components.items():\n",
    "            # Create figure\n",
    "            plt.figure(figsize=(10, 8), dpi=dpi)\n",
    "            \n",
    "            # Plot image\n",
    "            im = plt.imshow(data_array, cmap='viridis', vmin=v_min, vmax=v_max)\n",
    "            \n",
    "            # Add colorbar\n",
    "            cbar = plt.colorbar(im)\n",
    "            cbar.set_label(cbar_label, fontsize=12)\n",
    "            \n",
    "            # Add title and labels\n",
    "            plt.title(title, fontsize=14)\n",
    "            plt.xlabel('Space (x)', fontsize=12)\n",
    "            plt.ylabel('Space (y)', fontsize=12)\n",
    "            \n",
    "            # Tight layout\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save image\n",
    "            file_path = os.path.join(output_folder, f\"ffn_{comp_name}_timestep_{timestep}.png\")\n",
    "            plt.savefig(file_path, dpi=dpi, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            print(f\"Saved {comp_name} image to {file_path}\")\n",
    "\n",
    "    print(f\"All high-resolution images saved in folder: {output_folder}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # First train and save the model\n",
    "    # model_filename, mask_ratio, max_temp = train_and_save_model()\n",
    "    \n",
    "    # Then generate high-resolution images\n",
    "    generate_high_resolution_images(model_filename, mask_ratio, max_temp, vmin=295.0, vmax=335.0, dpi=600)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Total execution time: {elapsed_time:.2f} seconds ({elapsed_time/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8e4e8e",
   "metadata": {},
   "source": [
    "### For CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e0759f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import os\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Define grid dimensions\n",
    "grid_size = 100  # 100x100 grid\n",
    "\n",
    "# Define mask ratio parameter\n",
    "mask_ratio = 0.05  # 5% of data\n",
    "\n",
    "# Path configurations\n",
    "model_path = f'thermal_reconstruction_CNN_{mask_ratio}%.keras'\n",
    "data_path = r\"C:\\Users\\G_Modak\\Desktop\\Honeywell Project\\FEMap Models\\FAZ_FEModel Files\\Thermal analysis_temp on each nodes_different thickness inserts_cropped_100x100_please1.csv\"\n",
    "output_folder = \"high_res_thermal_images\"\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Load the trained model\n",
    "try:\n",
    "    autoencoder = load_model(model_path, compile=False)\n",
    "    print(f\"Model loaded successfully from {model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Couldn't load the model from {model_path}: {e}\")\n",
    "    print(\"Make sure to run the original training script first or adjust the model path.\")\n",
    "    exit()\n",
    "\n",
    "# Load the data\n",
    "try:\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"Data loaded successfully from {data_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Couldn't load the data from {data_path}: {e}\")\n",
    "    print(\"Please adjust the data path to point to your CSV file.\")\n",
    "    exit()\n",
    "\n",
    "# Extract the data values\n",
    "data = df.values\n",
    "\n",
    "# Get normalization parameters\n",
    "data_min = np.min(data)\n",
    "data_max = np.max(data)\n",
    "print(f\"Temperature range: {data_min:.2f} to {data_max:.2f}\")\n",
    "\n",
    "# Create a fixed mask for all samples (same as in training)\n",
    "fixed_mask = np.random.binomial(1, mask_ratio, (grid_size, grid_size))\n",
    "\n",
    "def prepare_data_for_timestamp(timestamp_idx):\n",
    "    \"\"\"Extract and prepare data for a specific timestamp\"\"\"\n",
    "    if timestamp_idx >= len(data):\n",
    "        print(f\"Warning: Timestamp {timestamp_idx} exceeds available data. Using modulo.\")\n",
    "        timestamp_idx = timestamp_idx % len(data)\n",
    "        \n",
    "    # Extract temperature field for the timestamp\n",
    "    temp_field = data[timestamp_idx].reshape(grid_size, grid_size)\n",
    "    \n",
    "    # Normalize the data\n",
    "    temp_field_norm = (temp_field - data_min) / (data_max - data_min)\n",
    "    \n",
    "    # Apply the mask to create sparse input\n",
    "    temp_field_masked = temp_field_norm * fixed_mask\n",
    "    \n",
    "    # Reshape for model input\n",
    "    model_input = temp_field_masked.reshape(1, grid_size, grid_size, 1)\n",
    "    \n",
    "    return temp_field, temp_field_masked.reshape(grid_size, grid_size), model_input\n",
    "\n",
    "def reconstruct_thermal_field(model, model_input):\n",
    "    \"\"\"Reconstruct a full thermal field from sparse measurements\"\"\"\n",
    "    # Make prediction\n",
    "    prediction = model.predict(model_input, verbose=0)\n",
    "    \n",
    "    # Extract and denormalize\n",
    "    reconstructed_norm = prediction[0, :, :, 0]\n",
    "    reconstructed = reconstructed_norm * (data_max - data_min) + data_min\n",
    "    \n",
    "    return reconstructed\n",
    "\n",
    "def generate_high_resolution_images(timestamp_idx, vmin=295.0, vmax=335.0, dpi=600):\n",
    "    \"\"\"Generate high-resolution images for a given timestamp\"\"\"\n",
    "    print(f\"Generating high-resolution images for timestamp {timestamp_idx}...\")\n",
    "    \n",
    "    # Prepare data for the timestamp\n",
    "    original, masked_normalized, model_input = prepare_data_for_timestamp(timestamp_idx)\n",
    "    \n",
    "    # Denormalize the masked data for visualization\n",
    "    masked = masked_normalized * (data_max - data_min) + data_min\n",
    "    \n",
    "    # Reconstruct the thermal field\n",
    "    reconstructed = reconstruct_thermal_field(autoencoder, model_input)\n",
    "    \n",
    "    # Calculate relative error\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        relative_error = (original - reconstructed) / original\n",
    "        # Handle division by zero or very small values\n",
    "        relative_error = np.where(np.isfinite(relative_error), relative_error, 0)\n",
    "    \n",
    "    # Calculate relative error bounds for better visualization\n",
    "    # Use 5th and 95th percentile to avoid extreme outliers\n",
    "    error_values = relative_error.flatten()\n",
    "    p05 = np.percentile(error_values, 5)\n",
    "    p95 = np.percentile(error_values, 95)\n",
    "    error_max = max(abs(p05), abs(p95))\n",
    "    \n",
    "    # Generate individual high-resolution images\n",
    "    image_types = {\n",
    "        'original': (original, 'Original Temperature Field', vmin, vmax, 'Temperature (K)'),\n",
    "        'sparse': (masked, f'Sparse Measurements ({mask_ratio*100}%)', vmin, vmax, 'Temperature (K)'),\n",
    "        'reconstructed': (reconstructed, 'Reconstructed Temperature Field', vmin, vmax, 'Temperature (K)'),\n",
    "        'relative_error': (relative_error, 'Relative Error ((Original-Reconstructed)/Original)', \n",
    "                          -error_max, error_max, 'Relative Temperature Difference')\n",
    "    }\n",
    "    \n",
    "    file_paths = {}\n",
    "    \n",
    "    for img_type, (data_array, title, v_min, v_max, cbar_label) in image_types.items():\n",
    "        # Create figure with larger size for high resolution\n",
    "        plt.figure(figsize=(10, 8), dpi=dpi)\n",
    "        \n",
    "        # Plot the image with viridis colormap\n",
    "        if img_type == 'relative_error':\n",
    "            im = plt.imshow(data_array, cmap='viridis', vmin=v_min, vmax=v_max)\n",
    "        else:\n",
    "            im = plt.imshow(data_array, cmap='viridis', vmin=v_min, vmax=v_max)\n",
    "        \n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(im)\n",
    "        cbar.set_label(cbar_label, fontsize=12)\n",
    "        \n",
    "        # Add title and labels\n",
    "        plt.title(title, fontsize=14)\n",
    "        plt.xlabel('Space (x)', fontsize=12)\n",
    "        plt.ylabel('Space (y)', fontsize=12)\n",
    "        \n",
    "        # Tight layout to maximize image size\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save high-resolution image\n",
    "        file_path = os.path.join(output_folder, f\"{img_type}_timestamp_{timestamp_idx}.png\")\n",
    "        plt.savefig(file_path, dpi=dpi, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        file_paths[img_type] = file_path\n",
    "        print(f\"Saved {img_type} image to {file_path}\")\n",
    "    \n",
    "    # Create a combined visualization for comparison\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 16), dpi=dpi)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Set titles for each subplot\n",
    "    titles = [\"Original Temperature Field\", \n",
    "              f\"Sparse Measurements ({mask_ratio*100}%)\",\n",
    "              \"Reconstructed Temperature Field\", \n",
    "              \"Relative Error ((Original-Reconstructed)/Original)\"]\n",
    "    \n",
    "    # Plot each image in the subplot\n",
    "    im0 = axes[0].imshow(original, cmap='viridis', vmin=vmin, vmax=vmax)\n",
    "    im1 = axes[1].imshow(masked, cmap='viridis', vmin=vmin, vmax=vmax)\n",
    "    im2 = axes[2].imshow(reconstructed, cmap='viridis', vmin=vmin, vmax=vmax)\n",
    "    im3 = axes[3].imshow(relative_error, cmap='viridis', vmin=-error_max, vmax=error_max)\n",
    "    \n",
    "    # Add colorbars\n",
    "    cbar0 = fig.colorbar(im0, ax=axes[0])\n",
    "    cbar0.set_label('Temperature (K)', fontsize=12)\n",
    "    cbar1 = fig.colorbar(im1, ax=axes[1])\n",
    "    cbar1.set_label('Temperature (K)', fontsize=12)\n",
    "    cbar2 = fig.colorbar(im2, ax=axes[2])\n",
    "    cbar2.set_label('Temperature (K)', fontsize=12)\n",
    "    cbar3 = fig.colorbar(im3, ax=axes[3])\n",
    "    cbar3.set_label('Relative Temperature Difference', fontsize=12)\n",
    "    \n",
    "    # Set titles\n",
    "    for i, title in enumerate(titles):\n",
    "        axes[i].set_title(title, fontsize=14)\n",
    "        axes[i].set_xlabel('Space (x)', fontsize=12)\n",
    "        axes[i].set_ylabel('Space (y)', fontsize=12)\n",
    "    \n",
    "    # Set the main title\n",
    "    plt.suptitle(f\"Thermal Field Reconstruction (Timestamp: {timestamp_idx})\", fontsize=16)\n",
    "    \n",
    "    # Adjust spacing between subplots\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    \n",
    "    # Save the combined figure\n",
    "    combined_file_path = os.path.join(output_folder, f\"combined_view_timestamp_{timestamp_idx}.png\")\n",
    "    plt.savefig(combined_file_path, dpi=dpi, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Saved combined image to {combined_file_path}\")\n",
    "    \n",
    "    return file_paths, combined_file_path\n",
    "\n",
    "# Generate high-resolution images for selected timestamps\n",
    "# You can change these to any timestamps you're interested in\n",
    "selected_timestamps = [0, 100, 200 ,300,400,500,600,700,800,900,1000]\n",
    "\n",
    "# Temperature range for visualization\n",
    "vmin = 295.0\n",
    "vmax = 335.0\n",
    "\n",
    "# Generate images for each selected timestamp\n",
    "for timestamp in selected_timestamps:\n",
    "    generate_high_resolution_images(timestamp, vmin=vmin, vmax=vmax, dpi=600)\n",
    "\n",
    "print(\"High-resolution image generation completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
